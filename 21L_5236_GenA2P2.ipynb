{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOpHAcGx9Zud"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision transformers datasets pillow matplotlib open_clip_torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Download COCO dataset (if not already downloaded)\n",
        "if not os.path.exists(\"val2017\"):\n",
        "    !wget http://images.cocodataset.org/zips/val2017.zip\n",
        "    !unzip -q val2017.zip\n",
        "\n",
        "# Path to images\n",
        "image_dir = \"val2017\"\n",
        "print(\"COCO dataset downloaded and extracted!\")\n"
      ],
      "metadata": {
        "id": "rYOHKO1WH2zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import open_clip\n",
        "\n",
        "# Load CLIP model and processor\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = open_clip.create_model_and_transforms(\"ViT-B/32\", pretrained=\"openai\")\n",
        "\n",
        "# Load tokenizer for text queries\n",
        "tokenizer = open_clip.get_tokenizer(\"ViT-B/32\")\n",
        "\n",
        "print(\"CLIP model loaded successfully!\")\n"
      ],
      "metadata": {
        "id": "9PRH2S97H6Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Function to preprocess and load images\n",
        "def load_images(image_dir, num_images=1000):  # Load first 1000 images\n",
        "    image_paths = sorted(os.listdir(image_dir))[:num_images]\n",
        "    images = [Image.open(os.path.join(image_dir, img)).convert(\"RGB\") for img in image_paths]\n",
        "    return images, image_paths\n",
        "\n",
        "# Load images\n",
        "images, image_filenames = load_images(image_dir)\n",
        "\n",
        "# Preprocess images for CLIP\n",
        "image_tensors = torch.stack([preprocess(img) for img in images]).to(device)\n",
        "\n",
        "print(f\"Loaded and preprocessed {len(images)} images.\")\n"
      ],
      "metadata": {
        "id": "gIrIc8QPIBpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode images into feature vectors\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_tensors)\n",
        "\n",
        "# Normalize features\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "print(\"Image embeddings generated!\")\n"
      ],
      "metadata": {
        "id": "Qgh6NcFoIDtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Function to search for similar images\n",
        "def search_images(query, top_k=5):\n",
        "    # Encode text\n",
        "    text_tokens = tokenizer([query]).to(device)\n",
        "    with torch.no_grad():\n",
        "        text_features = model.encode_text(text_tokens)\n",
        "\n",
        "    # Normalize text features\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Compute similarity scores (cosine similarity)\n",
        "    similarity_scores = (text_features @ image_features.T).squeeze(0)\n",
        "\n",
        "    # Get top-k most similar images\n",
        "    top_k_indices = similarity_scores.topk(top_k).indices.tolist()\n",
        "\n",
        "    return top_k_indices, similarity_scores[top_k_indices]\n",
        "\n",
        "# Test retrieval\n",
        "query = \"a person riding a bicycle\"\n",
        "top_indices, scores = search_images(query)\n",
        "\n",
        "print(\"Top matching images:\")\n",
        "for i, idx in enumerate(top_indices):\n",
        "    print(f\"{i+1}. {image_filenames[idx]} (Score: {scores[i]:.4f})\")\n"
      ],
      "metadata": {
        "id": "dglf-zPIIGVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to display images\n",
        "def display_results(query, top_indices, scores):\n",
        "    fig, axes = plt.subplots(1, len(top_indices), figsize=(15, 5))\n",
        "\n",
        "    for i, idx in enumerate(top_indices):\n",
        "        img = Image.open(os.path.join(image_dir, image_filenames[idx]))\n",
        "        axes[i].imshow(img)\n",
        "        axes[i].axis(\"off\")\n",
        "        axes[i].set_title(f\"Score: {scores[i]:.4f}\")\n",
        "\n",
        "    plt.suptitle(f\"Query: {query}\", fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "# Run retrieval and display\n",
        "display_results(query, top_indices, scores)\n"
      ],
      "metadata": {
        "id": "_XPk7fD-IIl7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}